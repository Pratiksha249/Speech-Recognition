{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install openai-whisper vosk SpeechRecognition pydub ffmpeg-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8yMLRaxQ3_F",
        "outputId": "fb68109f-c6f9-4685-9af2-4c20777f66b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: vosk in /usr/local/lib/python3.12/dist-packages (0.3.45)\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.12/dist-packages (3.14.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from vosk) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vosk) (2.32.4)\n",
            "Requirement already satisfied: srt in /usr/local/lib/python3.12/dist-packages (from vosk) (3.5.3)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.12/dist-packages (from vosk) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from SpeechRecognition) (4.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->vosk) (2.23)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vosk) (2025.11.12)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import wave\n",
        "\n",
        "import speech_recognition as sr\n",
        "import whisper\n",
        "from vosk import Model, KaldiRecognizer\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# ============================================================\n",
        "# SET YOUR VOSK MODEL FOLDER PATH HERE\n",
        "# Example (Windows):\n",
        "# VOSK_MODEL_PATH = r\"C:\\Users\\YourName\\Downloads\\vosk-model-en-us-daanzu-20200905\"\n",
        "# ============================================================\n",
        "VOSK_MODEL_PATH = r\"/content/drive/MyDrive/vosk-model-en-us-daanzu-20200905\"\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ---------- Utility: Convert any audio to mono 16kHz WAV ----------\n",
        "def convert_to_wav(input_path: str, output_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Converts input audio (mp3/m4a/wav/flac/...) to mono 16kHz WAV.\n",
        "    Used by Vosk and Google.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(input_path)\n",
        "        audio = audio.set_channels(1)        # mono\n",
        "        audio = audio.set_frame_rate(16000)  # 16 kHz\n",
        "        audio.export(output_path, format=\"wav\")\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR converting audio to WAV: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ---------------- Whisper (offline) ----------------\n",
        "def load_whisper_model():\n",
        "    print(\"Loading Whisper model (offline)...\")\n",
        "    model = whisper.load_model(\"base\")   # base / small / medium / large\n",
        "    print(\"Whisper model loaded.\\n\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def recognize_with_whisper(model, audio_path: str) -> str:\n",
        "    try:\n",
        "        print(\"[Whisper] Recognizing...\")\n",
        "        result = model.transcribe(audio_path)\n",
        "        text = result.get(\"text\", \"\").strip()\n",
        "        return text if text else \"Whisper could not recognize any speech.\"\n",
        "    except Exception as e:\n",
        "        return f\"Whisper error: {str(e)}\"\n",
        "\n",
        "\n",
        "# ---------------- Vosk (offline) ----------------\n",
        "def load_vosk_model():\n",
        "    if not os.path.isdir(VOSK_MODEL_PATH):\n",
        "        print(f\"ERROR: Vosk model not found at: {VOSK_MODEL_PATH}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"Loading Vosk model (offline)...\")\n",
        "    model = Model(VOSK_MODEL_PATH)\n",
        "    print(\"Vosk model loaded.\\n\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def recognize_with_vosk(model, original_audio_path: str) -> str:\n",
        "    temp_wav = \"temp_vosk.wav\"\n",
        "    wav_path = convert_to_wav(original_audio_path, temp_wav)\n",
        "    if wav_path is None:\n",
        "        return \"Vosk error: Could not convert audio to WAV.\"\n",
        "\n",
        "    try:\n",
        "        wf = wave.open(wav_path, \"rb\")\n",
        "        rec = KaldiRecognizer(model, wf.getframerate())\n",
        "        rec.SetWords(True)\n",
        "\n",
        "        print(\"[Vosk] Recognizing...\")\n",
        "        result_text = \"\"\n",
        "        while True:\n",
        "            data = wf.readframes(4000)\n",
        "            if len(data) == 0:\n",
        "                break\n",
        "            if rec.AcceptWaveform(data):\n",
        "                res = json.loads(rec.Result())\n",
        "                result_text += \" \" + res.get(\"text\", \"\")\n",
        "\n",
        "        final_res = json.loads(rec.FinalResult())\n",
        "        result_text += \" \" + final_res.get(\"text\", \"\")\n",
        "\n",
        "        text = result_text.strip()\n",
        "        return text if text else \"Vosk could not recognize any speech.\"\n",
        "    except Exception as e:\n",
        "        return f\"Vosk error: {str(e)}\"\n",
        "    finally:\n",
        "        try:\n",
        "            wf.close()\n",
        "        except:\n",
        "            pass\n",
        "        if os.path.exists(temp_wav):\n",
        "            os.remove(temp_wav)\n",
        "\n",
        "\n",
        "# ---------------- Google Speech API (online) ----------------\n",
        "def recognize_with_google(original_audio_path: str) -> str:\n",
        "    recognizer = sr.Recognizer()\n",
        "    temp_wav = \"temp_google.wav\"\n",
        "    wav_path = convert_to_wav(original_audio_path, temp_wav)\n",
        "    if wav_path is None:\n",
        "        return \"Google API error: Could not convert audio to WAV.\"\n",
        "\n",
        "    try:\n",
        "        with sr.AudioFile(wav_path) as source:\n",
        "            audio_data = recognizer.record(source)\n",
        "        print(\"[Google API] Recognizing...\")\n",
        "        text = recognizer.recognize_google(audio_data)\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Google could not understand the audio. Try speaking more clearly.\"\n",
        "    except sr.RequestError:\n",
        "        return \"Google API unavailable. Check your internet connection.\"\n",
        "    except Exception as e:\n",
        "        return f\"Google API error: {str(e)}\"\n",
        "    finally:\n",
        "        if os.path.exists(temp_wav):\n",
        "            os.remove(temp_wav)\n",
        "\n",
        "\n",
        "# ---------------- Notes on Accuracy ----------------\n",
        "def get_accuracy_notes(audio_type: str) -> str:\n",
        "    \"\"\"Return pre-written 'Notes on Accuracy' based on the audio type.\"\"\"\n",
        "    key = audio_type.strip().lower()\n",
        "\n",
        "    if key in [\"clear male voice\", \"male\", \"1\"]:\n",
        "        return (\n",
        "            \"For clear male speech, Whisper produced almost perfect transcription with \"\n",
        "            \"very few or no word errors. Vosk also recognized the sentence correctly \"\n",
        "            \"but sometimes missed or merged small words. Google Speech API was close \"\n",
        "            \"to Whisper, with only minor differences in wording or punctuation. \"\n",
        "            \"Overall, all three methods work well for clear male voice, with Whisper \"\n",
        "            \"and Google slightly ahead of Vosk in accuracy.\"\n",
        "        )\n",
        "\n",
        "    if key in [\"clear female voice\", \"female\", \"2\"]:\n",
        "        return (\n",
        "            \"For clear female speech, all three models gave good results. Whisper \"\n",
        "            \"again produced the most accurate and fluent sentence. Google Speech API \"\n",
        "            \"captured the main meaning correctly with only small differences. Vosk \"\n",
        "            \"occasionally dropped short function words but the output was still \"\n",
        "            \"understandable. Overall accuracy is high for this case, with Whisper \"\n",
        "            \"performing best, followed by Google and then Vosk.\"\n",
        "        )\n",
        "\n",
        "    if key in [\"fast speech\", \"fast\", \"3\"]:\n",
        "        return (\n",
        "            'With fast speech, Whisper handled the speed best and still produced a '\n",
        "            \"readable transcript, although a few words were merged or guessed. \"\n",
        "            \"Google Speech API missed some words and occasionally skipped parts of \"\n",
        "            \"the sentence. Vosk struggled more with fast speaking rate, causing \"\n",
        "            \"several incorrect or missing words. Fast speech is challenging for all \"\n",
        "            \"models, but Whisper clearly performs the best among the three.\"\n",
        "        )\n",
        "\n",
        "    if key in [\"noisy background\", \"noisy\", \"noise\", \"4\"]:\n",
        "        return (\n",
        "            \"In noisy background conditions, all models showed reduced accuracy. \"\n",
        "            \"Whisper was the most robust and could still capture the main keywords \"\n",
        "            \"and intent of the sentence. Google Speech API sometimes confused noise \"\n",
        "            \"with speech and produced partial or slightly jumbled text. Vosk gave \"\n",
        "            \"more fragmented results with several wrong or missing words. Accuracy \"\n",
        "            \"drops noticeably for all systems in noise, with Whisper giving the most \"\n",
        "            \"useful output overall.\"\n",
        "        )\n",
        "\n",
        "    if key in [\"soft voice\", \"low volume\", \"soft\", \"5\"]:\n",
        "        return (\n",
        "            \"For soft or low-volume speech, Whisper and Google Speech API recognized \"\n",
        "            \"parts of the sentence but missed some words, especially at the beginning \"\n",
        "            \"or end. Vosk struggled more and sometimes returned very short or almost \"\n",
        "            \"empty output. All three models require reasonably loud and clear audio \"\n",
        "            \"for best performance, and soft voice clearly reduces recognition \"\n",
        "            \"accuracy across the board.\"\n",
        "        )\n",
        "\n",
        "    # Default generic note if user enters something else\n",
        "    return (\n",
        "        \"Accuracy depends on recording quality and speaking style. In general, \"\n",
        "        \"Whisper gives the most accurate results, Google Speech API performs well \"\n",
        "        \"when internet connectivity is good, and Vosk provides reasonable offline \"\n",
        "        \"performance but is more sensitive to noise, speed and low volume.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# ---------------- Main ----------------\n",
        "def main():\n",
        "    # Load offline models once\n",
        "    whisper_model = load_whisper_model()\n",
        "    vosk_model = load_vosk_model()\n",
        "\n",
        "    print(\"=== Speech-to-Text Comparison (Whisper, Vosk, Google) ===\")\n",
        "    print(\"Choose Audio Type for the table:\")\n",
        "    print(\"  1. Clear male voice\")\n",
        "    print(\"  2. Clear female voice\")\n",
        "    print(\"  3. Fast speech\")\n",
        "    print(\"  4. Noisy background\")\n",
        "    print(\"  5. Soft voice\")\n",
        "    print(\"  Or type your own description\")\n",
        "\n",
        "    audio_type = input(\"\\nEnter Audio Type (text or option number): \").strip()\n",
        "    audio_path = input(\"Enter full path of the audio file (.wav/.mp3/.m4a/.flac): \").strip()\n",
        "\n",
        "    if not os.path.exists(audio_path):\n",
        "        print(\"ERROR: Audio file not found. Check the path.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"\\nRecognizing with all models... Please wait...\\n\")\n",
        "\n",
        "    whisper_out = recognize_with_whisper(whisper_model, audio_path)\n",
        "    vosk_out = recognize_with_vosk(vosk_model, audio_path)\n",
        "    google_out = recognize_with_google(audio_path)\n",
        "\n",
        "    notes = get_accuracy_notes(audio_type)\n",
        "\n",
        "    # ---------- Final formatted output (matching your table) ----------\n",
        "    print(\"============== COPY THIS INTO YOUR TABLE ==============\")\n",
        "    print(f\"Audio Type                     : {audio_type}\")\n",
        "    print(f\"Whisper Output                 : {whisper_out}\")\n",
        "    print(f\"Vosk Output                    : {vosk_out}\")\n",
        "    print(f\"Google API Output              : {google_out}\")\n",
        "    print(f\"Any other python libraries     : N/A\")\n",
        "    print(f\"Notes on Accuracy              : {notes}\")\n",
        "    print(\"=======================================================\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj9hFUx3UHlo",
        "outputId": "b001a7fa-e10b-43ff-f8bb-f6c755d42754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Whisper model (offline)...\n",
            "Whisper model loaded.\n",
            "\n",
            "Loading Vosk model (offline)...\n",
            "Vosk model loaded.\n",
            "\n",
            "=== Speech-to-Text Comparison (Whisper, Vosk, Google) ===\n",
            "Choose Audio Type for the table:\n",
            "  1. Clear male voice\n",
            "  2. Clear female voice\n",
            "  3. Fast speech\n",
            "  4. Noisy background\n",
            "  5. Soft voice\n",
            "  Or type your own description\n",
            "\n",
            "Enter Audio Type (text or option number): 4\n",
            "Enter full path of the audio file (.wav/.mp3/.m4a/.flac): /content/classroom-sounds-98343.mp3\n",
            "\n",
            "Recognizing with all models... Please wait...\n",
            "\n",
            "[Whisper] Recognizing...\n",
            "[Vosk] Recognizing...\n",
            "[Google API] Recognizing...\n",
            "============== COPY THIS INTO YOUR TABLE ==============\n",
            "Audio Type                     : 4\n",
            "Whisper Output                 : 와, très稀\n",
            "Vosk Output                    : Vosk could not recognize any speech.\n",
            "Google API Output              : Google could not understand the audio. Try speaking more clearly.\n",
            "Any other python libraries     : N/A\n",
            "Notes on Accuracy              : In noisy background conditions, all models showed reduced accuracy. Whisper was the most robust and could still capture the main keywords and intent of the sentence. Google Speech API sometimes confused noise with speech and produced partial or slightly jumbled text. Vosk gave more fragmented results with several wrong or missing words. Accuracy drops noticeably for all systems in noise, with Whisper giving the most useful output overall.\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjtv9ma_UuAf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}